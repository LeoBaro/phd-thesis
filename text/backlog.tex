In synchronous deep learning systems, computations in workers occur synchronously with respect to the shared model state



For production environments, recovery options in case machine failure during traini


% A distributed deep learning system has to be particularily hardened to avoid loosing the training progress of weeks or months of has to implement methods to 


EXCURSION!
Understanding the network overhead.


% TODO: HMMM!!!! Where should this one have gone.
% Last but not least, a cluster may provide various services that are not all necessarily dedicated to the deep learning task at hand (e.g. the hosting of files or other applications, etc.).


SYSTEM PERSPECTIVE: DO WE HAVE REDUNANCE?  Also note that the optimizer exists only once in our simplified system. In practice this means that the cluster may not be able to continue training where it left off if that node dies. Snapshot?




OTHER TOPICS
Using a simple real numbers is the most generic method is to describe the weight ($W$) of a neuron connection. Thus, on a PC 4 bytes (standard single precision IEEE floating point format) are required to store a single weight. However, it should be noted that extensive work exists that explores more efficient encodings \cite{a,b,c}. 


Langer et al.


Improvement of parameter server using n:k communication.




% Of course, it is possible to overcome almost any memory constraint using swapping and streaming from disk techniques. However, disk-access is comparatively slow. Furthermore, it requires resources and, thus, reduces the time available to perform actual model optimization. Furthermore, the capabilities of the hardware (=GPUs) put tight limits the amount of computational resources that can be efficiently operated within a single integrated system. Even if the model and the intermediate inputs of all neurons and the dataset size all are within manageable limits, it might still take weeks or months to train a complex model to the desired quality. Distributed deep learning systems try to speedup the training time by harnessing the capabilities of multiple devices simultaneously.


%Note that while we presented scaling up and scaling out as competing ideas in \autoref{s:Introduction}, they rather represent two different scaling dimensions. In practice both approaches are orthogonal. Each additional network node in a scaling out solution incurs not only a technical overhead, but also an administrative overhead, which puts system builders, whose goal is to finetune cluster setups in other to maximize value, in a constant dilemma.



Whether distributed deep learning system can be operated effectively depends on their communication demands. GPU-accelerated deep learning has shown to be a major game changer in this context, since it increases the computation speed of individual nodes by orders of magnitude \cite{Langer}. In Thus, the performance critical measure is typically the relative spread between the size of and frequency of data transmissions and the time required to compute or update the model (\autoref{s:Understanding-the-network}).
Faster progression of computation can raise needs requires the need of more frequent . Thus, , which 
which in fact might relate to the computational abilities of the cluster nodes and the network bandwidth \cite{Langer}.  


Revisiting Synchronous Gradient Descent
-----------------------------------------
Both Dean et al. (2012) and Chilimbi et al. (2014) use versions of Async-SGD where the main po- tential problem is that each worker computes gradients over a potentially old version of the model.

To alleviate the straggler problem, we introduce backup workers (Dean&Barroso, 2013)
