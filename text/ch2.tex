\noindent 
In this chapter, we will provide an overview of neural networks and deep learning, including their fundamental concepts, architecture, and training methods. We will also discuss the specific ways in which these techniques have been applied to astrophysical problems, highlighting their successes and limitations.

\section{Introduction to Gamma-Ray Astronomy}
\label{s:Introduction-to-Gamma-Ray-Astronomy}

Gamma-ray astronomy studies the most energetic electromagnetic radiation in the universe, with energies ranging from a few hundred keV to several TeV. These rays are produced by some of the most extreme and violent processes in the universe, including supernovae, black holes, and neutron stars.
The gamma-ray sky is the portion of the universe that is observable at gamma-ray energies, typically defined as the energy range of a few thousand to a few trillion electron volts. The study of the gamma-ray sky allows us to probe some of the most extreme environments in the universe, such as supernovae, black holes, and neutron stars \cite{Fishman1995}.
Gamma rays are the most energetic form of light, and they are produced by accelerating charged particles, such as electrons or protons, to very high energies. These particles can be accelerated through various processes, including the collapse of massive stars, the acceleration of particles in magnetic fields, and the collision of particles in high-energy environments. The study of gamma rays, therefore, provides a unique window into the physics of these extreme environments.
The gamma-ray sky is dominated by two main sources: known sources and the diffuse gamma-ray background \cite{Ackermann2015}. Known sources are individual objects or phenomena that have been identified and studied in detail, such as active galactic nuclei (AGN), pulsars, and supernovae \cite{Kelley2020}. These sources are relatively bright and can be easily detected by gamma-ray telescopes \cite{Abdo}.
The diffuse gamma-ray background, on the other hand, is a faint and diffuse emission that is present throughout the entire gamma-ray sky \cite{Ackermann2015}. This background emission is thought to be produced by the collective emission of many faint sources that are too faint to be detected individually \cite{Abdo}. It is also thought to be produced by the interaction of cosmic rays with the interstellar medium (Strong et al., 2010), as well as by the decay of radioactive isotopes \cite{Kelley2020}.
One of the main challenges in studying the gamma-ray sky is the difficulty in separating the contribution of known sources from the diffuse background emission \cite{Ackermann2015}. This is because the background emission is much brighter than the individual sources, making it difficult to study them in detail \cite{Abdo}. To overcome this challenge, astronomers use various techniques, such as spatial and spectral analysis \cite{Kelley2020}, to separate the contribution of known sources from the background emission \cite{Ackermann2015}.
Gamma-Ray telescopes can be divided into two main categories: space-based telescopes, which are placed in orbit around the Earth, and ground-based telescopes, which are located on the surface of the Earth and observe gamma rays through the atmosphere. 

The study of gamma-rays has a long history, dating back to the discovery of cosmic rays by Victor Hess in 1912. However, it was not until the development of space-based instruments in the 1960s and 1970s that gamma-ray astronomy became a mainstream field of study. One of the first satellite missions to make significant contributions to the field was the Gamma Ray Observatory (GRO), launched by NASA in 1991. GRO carried a suite of instruments designed to detect and measure gamma-rays from a variety of sources, including the Crab Nebula, the Cygnus X-1 binary system, and the Galactic Center \cite{fichtel1994egret}.

In the decades since the launch of GRO, advances in detector technology and instrumentation have allowed for the development of increasingly sensitive gamma-ray telescopes. These include the Fermi Gamma-ray Space Telescope, launched by NASA in 2008, and the Cherenkov Telescope Array (CTA), an international collaboration that is currently under construction. These telescopes have provided valuable insights into the physics of high-energy phenomena and have led to the discovery of many new gamma-ray sources.


\section{Gamma Ray Bursts}
\label{s:Gamma-Ray-Bursts}
Gamma Ray Bursts (GRBs) are some of the most powerful and mysterious phenomena in the universe. These intense bursts of gamma rays, the most energetic form of light, are thought to be caused by the collapse of massive stars or the merger of neutron stars or black holes. GRBs are extremely bright and can be detected from billions of light-years away, making them some of the most distant objects we can observe.
Despite their brightness, GRBs are difficult to detect and study because they are short-lived and unpredictable. GRBs typically last only a few seconds to a few minutes, and they can occur at any time and in any direction. This makes it challenging to point a telescope at a GRB to study it quickly.
To study GRBs, astronomers use a variety of telescopes and techniques, including space-based telescopes such as the Fermi Gamma-ray Space Telescope, the Swift Gamma-Ray Burst Mission, and the Agile Gamma-ray Space Telescope, as well as ground-based telescopes such as the Major Atmospheric Gamma Imaging Cherenkov Telescopes (MAGIC), the The High Energy Stereoscopic System (HESS), and Large-Sized Telescope (LST-1). 
The study of GRBs has led to many important discoveries in astrophysics and cosmology. For example, GRBs have been used to measure the expansion rate of the universe and to study the properties of black holes and neutron stars. They have also provided valuable information about the properties and evolution of the universe in the early stages of its development.

\section{Multi-wavelenght and multi-messanger astronomy}
Multi-wavelength and multi-messenger astronomy is a rapidly developing field that studies celestial objects and phenomena using a wide range of electromagnetic and non-electromagnetic radiation. This includes everything from radio waves and visible light to gamma rays and gravitational waves.
One of the key advantages of multi-wavelength and multi-messenger astronomy is that it allows astronomers to study the universe from a more comprehensive and holistic perspective. By combining data from different wavelengths and messengers, astronomers can gain a better understanding of the physical processes at work in the universe.
One of the most significant developments in multi-wavelength and multi-messenger astronomy has been detecting gravitational waves, ripples in space-time predicted by Einstein's theory of general relativity. These waves are produced by the acceleration of massive objects, such as the merging of binary black holes or neutron stars. The first gravitational wave detection was made in 2015 by the Laser Interferometer Gravitational-Wave Observatory (LIGO). Since then, several other gravitational wave detections have been made, opening up a new window into the universe \cite{abbott2016observation}.
Another important aspect of multi-wavelength and multi-messenger astronomy is the study of transient phenomena, such as supernovae, gamma-ray bursts, and fast radio bursts. These events are often brief and elusive, and can be studied more effectively by combining data from multiple telescopes and instruments. For example, the combination of data from X-ray, optical, and radio telescopes has allowed for the study of the afterglows of gamma-ray bursts (GRBs), which are thought to be the result of the collapse of massive stars or the merging of binary neutron stars \cite{Fishman1995}.

\section{Ground-Based Gamma-Ray Imaging with Cherenkov Telescopes}
Cherenkov telescopes are designed to detect the Cherenkov radiation produced when high-energy particles, such as gamma rays, pass through the Earth's atmosphere. When a high-energy particle travels through the atmosphere, it can produce a shower of lower-energy particles, producing a faint, blue light called Cherenkov radiation. This radiation is produced when the charged particles in the shower move faster than the speed of light in the medium (usually air or water) \cite{ong2009gamma}. 
Cherenkov telescopes are typically composed of large mirrors and a detector array. The mirrors focus the Cherenkov radiation onto the detector array, typically composed of photomultiplier tubes or charged coupled devices. These detectors measure the intensity and arrival direction of the Cherenkov radiation, allowing astronomers to create images and maps of the gamma-ray sky.
One of the most well-known Cherenkov telescope arrays is the Very Energetic Radiation Imaging Telescope Array System (VERITAS), located in Arizona, USA. The VERITAS array consists of four 12-meter telescopes, which detect gamma-rays with energies ranging from a few hundred GeV to several TeV. The VERITAS array has made many important discoveries, including the detection of gamma-ray emission from supernova remnants and the discovery of a new class of gamma-ray emitting objects called "gamma-ray flatspectrum radio quasars" \cite{weekes2002very}.
Another important Cherenkov telescope array is the High Energy Stereoscopic System (H.E.S.S.), located in Namibia, Africa. The H.E.S.S. array consists of five 12-meter telescopes, which detect gamma-rays with energies ranging from a few hundred GeV to several TeV. The H.E.S.S. array has made many important discoveries, including the detection of gamma-ray emission from the center of the Milky Way galaxy and the discovery of a new class of gamma-ray emitting objects called "dark accelerators" \cite{aharonian2004high}

\section{The Cherenkov Telescope Array Observatory}
\label{CTA}
The Cherenkov Telescope Array Observatory (CTA) is a next-generation ground-based gamma-ray observatory currently under development. CTA is a collaboration of over 1200 scientists from 32 countries and is expected to be the most sensitive and highest-resolution gamma-ray observatory ever built \cite{actis2011design}.
CTA will consist of two arrays of telescopes, one located in the northern hemisphere (CTA-North) and one in the southern hemisphere (CTA-South). Each array will contain around 100 telescopes, ranging from 4 to 23 meters in diameter. CTA will be able to detect gamma rays with energies ranging from a few tens of GeV to several TeV and will have a sensitivity that is 10 to 100 times higher than current gamma-ray observatories \cite{actis2011design}.
One of the main science goals of CTA is to study the most extreme and violent phenomena in the universe, such as supernovae, black holes, and gamma-ray bursts. CTA will also be used to study the properties of dark matter, search for signatures of new physics beyond the Standard Model, and study the cosmic microwave background and the early universe \cite{acharya2013introducing}.
CTA will also be used to study the gamma-ray emission from the center of the Milky Way galaxy and other nearby galaxy clusters. These studies will help astronomers understand the processes that produce gamma-rays in these regions, and will provide important insights into the nature of black holes and the acceleration of high-energy particles.
Construction of CTA is expected to be completed in the mid-2020s, and the observatory is expected to be fully operational for at least 20 years. CTA is expected to be a major step forward in gamma-ray astronomy and will play a crucial role in advancing our understanding of the most extreme and violent phenomena in the universe \cite{the2018cherenkov}.

\subsection{The Science Alert Generation system}

\subsection{The Li&Ma standard technique for the real time analysis}
\label{sb:li-ma}

\section{Neural Networks and Deep Learning}
\label{s:Neural-Networks-and-Deep-Learning}

Neural networks and deep learning have become increasingly popular techniques in astrophysics in recent years. These methods, which are based on the structure and function of the human brain, have shown great promise in various applications, including data analysis, image classification, and prediction of physical phenomena. At their core, neural networks are a type of machine learning algorithm that is designed to recognize patterns in data. They consist of multiple interconnected "neurons," which are inspired by the neurons in the human brain. These neurons process input data and transmit the result to other neurons in the network until the final output is produced. Deep learning is a subset of machine learning that involves using deep neural networks, which are neural networks with many layers (hence the term "deep"). These deep networks can learn more complex patterns in data than shallow networks, allowing them to achieve higher levels of accuracy in tasks such as image recognition and natural language processing.

\subsubsection{Architectures}
There are several different types of neural network architectures, including feedforward networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).... Each type of architecture is suited to different types of tasks and data. Feedforward networks are the most basic type of neural network, in which the data flows through the network in a single direction, from the input layer to the output layer. These networks are well-suited to tasks such as classification, where the output is a discrete label or category. CNNs are specifically designed for processing data with a grid-like structure, such as images. They consist of multiple "convolutional" layers, which apply filters to the input data to extract features. These features are then passed through additional network layers to perform the final task, such as image classification. RNNs are designed to process sequential data, such as time series or natural language. They contain "recurrent" connections, which allow the network to use information from previous time steps to inform its processing at the current time step. This makes RNNs well-suited to tasks such as language translation or speech recognition.

\subsubsection{Training}
Neural networks are trained using a process called "backpropagation," in which the network's predictions are compared to the true labels of the data, and the error is propagated back through the network to update the neurons' weights. This process is repeated for multiple epochs until the network reaches a satisfactory level of accuracy.

\subsubsection{Applications in Astrophysics}
Neural networks and deep learning have been applied to many astrophysical problems, including data analysis of observations from telescopes and simulations of physical phenomena. 
One example of deep learning in this field is the application of convolutional neural networks (CNNs) to classify gamma-ray images from instruments. In \cite{dieleman2015galaxy}, the authors used CNNs to classify galaxy images as either spiral or elliptical based on morphological features, achieving an accuracy of 95\%. In \cite{lopez2018automatic}, the authors trained a CNN to classify gamma-ray sources as either point-like or extended, seen by the Fermi Gamma-ray Space Telescope, achieving an accuracy of 95\%. Another example is the use of deep learning to improve the accuracy of physical models in gamma-ray astrophysics. In \cite{perez2019deep}, the authors used a deep neural network to model the emission from relativistic jets in active galactic nuclei, which are powerful sources of gamma rays. By incorporating additional physical constraints into the training process, the authors improved the accuracy of their model, demonstrating the potential of deep learning to enhance our understanding of these complex systems.

\section{Anomaly Detection}
Anomaly detection involves the identification of patterns or events in data that are unusual or unexpected compared to a baseline, or normal behavior  \cite{chandola2009anomaly}. Various factors, such as errors in data collection, rare events, or the influence of external factors, can cause these anomalies.
There are several different approaches to anomaly detection, including statistical, machine learning, and data mining methods. Statistical methods involve statistical tests to identify anomalies. In contrast, machine learning methods involve using algorithms trained to recognize patterns in data and identify anomalies based on those patterns. Data mining methods involve techniques such as clustering or classification to identify unusual patterns in data.
One approach to performing anomaly detection in time series data is to use an autoencoder in a semi-supervised setting. Advantage of semi-supervised setting, [todo] As stated before, an autoencoder is a type of neural network trained to reconstruct its input data. It consists of an encoder, which maps the input data to a lower-dimensional representation, and a decoder, which maps the lower-dimensional representation back to the original data space.The autoencoder is trained to reconstruct normal data points, while anomalous data points are less well reconstructed. The reconstruction error, which measures the difference between the input data and the reconstructed data, can then be used to identify anomalous data points. There have been several scientific papers on using autoencoders for anomaly detection in time series data. Some examples include: \cite{wang2020anomaly}, \cite{malhotra2016anomaly}, and \cite{zimmerer2018unsupervised}. Anomaly detection techniques, including those based on autoencoders, have been used in gamma-ray astronomy for identifying unusual patterns or deviations in gamma-ray data. Autoencoder-based anomaly detection techniques have been applied to gamma-ray data in several studies. For example, in "Anomaly Detection in Gamma-Ray Astronomy with Deep Learning" by \cite{fabbro2019anomaly}, the authors demonstrate the use of autoencoders for identifying anomalous events in gamma ray data collected by the Fermi Large Area Telescope (LAT). They show that the autoencoder-based approach effectively identifies anomalous events that are not detected by traditional methods. Another example is \cite{guo2020anomaly}, which proposes to use variational autoencoders. The authors evaluate the method's performance on simulated gamma-ray data and show that it can effectively identify anomalous events. 






\section{Summary}
\label{s:Background-Summary}

The final section of each chapter should summarize the chapter. In comparison to the chapter, the summary should be short ($\frac{1}{2}$ to $2$ pages is normal).